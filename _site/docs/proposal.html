<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Project Proposal | viar.github.io</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Project Proposal" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Visually Impaired Accessible Room" />
<meta property="og:description" content="Visually Impaired Accessible Room" />
<meta property="og:site_name" content="viar.github.io" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Project Proposal" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Visually Impaired Accessible Room","headline":"Project Proposal","url":"/docs/proposal.html"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=362de996cb0cfcff14396c775f98d56bdf483776">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <div class="container-lg px-3 my-5 markdown-body">
      
      <h1><a href="/">viar.github.io</a></h1>
      

      <h1 id="project-proposal">Project Proposal</h1>

<h2 id="1-motivation--objective">1. Motivation &amp; Objective</h2>

<p>This project aims to assist visually impaired individuals in achieving spatial awareness and navigational guidance. By combining a depth camera, Apple Watch, and auditory feedback, the system provides precise object retrieval instructions. The Apple Watch delivers haptic feedback, while auditory cues offer real-time navigation and guidance, enhancing independence in locating and grasping objects.</p>

<h2 id="2-state-of-the-art--its-limitations">2. State of the Art &amp; Its Limitations</h2>

<p>Currently, assistive technologies like Seeing AI, Autour, and BlindSquare provide limited navigational or object detection assistance using either vision or audio alone. Head-mounted depth cameras have been explored but can be cumbersome, and current systems often lack intuitive, multi-modal feedback for accurate object retrieval.</p>

<h2 id="3-novelty--rationale">3. Novelty &amp; Rationale</h2>

<p>Our approach integrates a stationary depth camera (RealSense L515), Apple Watch haptic feedback, and auditory cues to provide a more intuitive and comfortable solution. By leveraging stationary depth sensing and hands-free, multi-modal feedback, this system promises higher accuracy in navigation and object retrieval, without the discomfort of wearable headgear. We believe this comprehensive setup will significantly improve user experience and reliability.</p>

<h2 id="4-potential-impact">4. Potential Impact</h2>

<p>If successful, this system will provide a major improvement in spatial awareness and object retrieval for visually impaired individuals. Technically, it will demonstrate the integration of depth sensing, haptic feedback, and auditory guidance. Broadly, it has the potential to enhance accessibility tools, offering users greater autonomy and confidence in navigating their environments.</p>

<h2 id="5-challenges">5. Challenges</h2>

<p>Key challenges include:</p>
<ul>
  <li>Ensuring real-time, accurate object detection and localization.</li>
  <li>Developing intuitive haptic and auditory feedback that is easy for users to interpret.</li>
  <li>Overcoming limitations in depth camera performance under varying lighting and room layouts.</li>
</ul>

<h2 id="6-requirements-for-success">6. Requirements for Success</h2>

<p>Necessary skills and resources:</p>
<ul>
  <li><strong>Technical Skills</strong>: Proficiency in computer vision, haptic feedback design, and real-time processing.</li>
  <li><strong>Hardware</strong>: RealSense L515 depth camera, Apple Watch, and audio feedback devices (e.g., AirPods).</li>
  <li><strong>Software</strong>: Object recognition, path planning, and natural language processing algorithms.</li>
</ul>

<h2 id="7-metrics-of-success">7. Metrics of Success</h2>

<p>Metrics for success include:</p>
<ul>
  <li><strong>Accuracy</strong>: Correctly identified and localized objects.</li>
  <li><strong>Response Time</strong>: Speed from user request to successful object retrieval.</li>
  <li><strong>User Satisfaction</strong>: Feedback from visually impaired users testing the prototype.</li>
  <li><strong>Reliability</strong>: Consistency across different environments and conditions.</li>
  <li><strong>Ease of Use</strong>: Time taken by users to learn and effectively use the system.</li>
</ul>

<h2 id="8-execution-plan">8. Execution Plan</h2>

<h3 id="key-tasks">Key Tasks</h3>
<ol>
  <li>Set up depth camera and Apple Watch integration.</li>
  <li>Develop object detection and tracking algorithms.</li>
  <li>Implement haptic and auditory feedback systems.</li>
  <li>Conduct user testing and iterate on feedback.</li>
</ol>

<h3 id="team-task-partitioning">Team Task Partitioning</h3>
<ul>
  <li><strong>Jason Wu</strong>: Depth camera setup and path planning.</li>
  <li><strong>Team Members</strong>: Object detection, haptic feedback programming, and user testing.</li>
</ul>

<h2 id="9-related-work">9. Related Work</h2>

<h3 id="9a-papers">9.a. Papers</h3>
<ol>
  <li>
    <p><em>Using Depth Cameras for Object Detection and Navigation Assistance for the Visually Impaired</em><br />
Discusses mapping environments and aiding in object detection using depth cameras.</p>
  </li>
  <li>
    <p><em>Haptic Feedback for Object Localization and Grasping in Assistive Technologies</em><br />
Explores the role of haptic feedback in guiding users toward objects accurately.</p>
  </li>
  <li>
    <p><em>Speech Recognition Systems in Assistive Technologies: Navigating and Retrieving Objects</em><br />
Reviews voice-controlled systems for navigation and object interaction.<br />
<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9795125">Link</a><br />
<a href="https://dl.acm.org/doi/abs/10.1145/2982142.2982160">Link</a></p>
  </li>
  <li>
    <p><em>Moving Object Detection in RGBD Data</em><br />
Highlights techniques for background subtraction and motion detection using RGB-D cameras.<br />
<a href="https://www.mdpi.com/2313-433X/4/5/71">Link</a></p>
  </li>
  <li>
    <p><em>Holistic Scene Understanding for 3D Object Detection with RGBD Cameras</em><br />
Discusses methods for integrating RGB and depth data for indoor environments.<br />
<a href="https://openaccess.thecvf.com/content_iccv_2013/papers/Lin_Holistic_Scene_Understanding_2013_ICCV_paper.pdf">Link</a></p>
  </li>
</ol>

<h3 id="9b-datasets">9.b. Datasets</h3>
<ol>
  <li>
    <p><strong>Intel RealSense Path Planning Dataset</strong><br />
<a href="https://github.com/pancx/pathplanning">Link</a></p>
  </li>
  <li>
    <p><strong>Object Detection and Navigation Dataset</strong><br />
Custom dataset based on user testing and room mapping data.</p>
  </li>
</ol>

<h3 id="9c-software">9.c. Software</h3>
<ol>
  <li>
    <p><strong>Intel RealSense SLAM Library</strong>: For real-time object localization.<br />
<a href="https://www.jstage.jst.go.jp/article/jsp/23/4/23_201/_pdf/-char/en">Link</a></p>
  </li>
  <li>
    <p><strong>Whisper OpenAI</strong>: For speech recognition and natural language processing.<br />
<a href="https://developer.apple.com/tutorials/app-dev-training/transcribing-speech-to-text">Link</a></p>
  </li>
</ol>

<h2 id="10-references">10. References</h2>

<ol>
  <li>
    <p><em>Using Depth Cameras for Object Detection and Navigation Assistance for the Visually Impaired</em><br />
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9795125</p>
  </li>
  <li>
    <p><em>Haptic Feedback for Object Localization and Grasping in Assistive Technologies</em><br />
https://dl.acm.org/doi/abs/10.1145/2982142.2982160</p>
  </li>
  <li>
    <p><em>Moving Object Detection in RGBD Data</em><br />
https://www.mdpi.com/2313-433X/4/5/71</p>
  </li>
  <li>
    <p><em>Holistic Scene Understanding for 3D Object Detection with RGBD Cameras</em><br />
https://openaccess.thecvf.com/content_iccv_2013/papers/Lin_Holistic_Scene_Understanding_2013_ICCV_paper.pdf</p>
  </li>
  <li>
    <p><em>FusionVision: 3D Object Reconstruction with RGB-D Cameras</em><br />
https://www.mdpi.com/1424-8220/24/9/2889</p>
  </li>
  <li>
    <p><strong>Intel RealSense SLAM Library</strong><br />
https://www.jstage.jst.go.jp/article/jsp/23/4/23_201/_pdf/-char/en</p>
  </li>
  <li>
    <p><strong>Whisper OpenAI</strong><br />
https://developer.apple.com/tutorials/app-dev-training/transcribing-speech-to-text</p>
  </li>
</ol>


      
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.0/anchor.min.js" integrity="sha256-lZaRhKri35AyJSypXXs4o6OPFTbTmUoltBbDCbdzegg=" crossorigin="anonymous"></script>
    <script>anchors.add();</script>
  </body>
</html>
